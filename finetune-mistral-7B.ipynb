{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5875ff78-ce37-4f8d-a550-7cd9dd8044e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入常用模块\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a6c82b-4e28-42fd-b5c6-1ea2fddf4f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/featurize'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ec6869-a7fa-4f18-9188-78bf23dc66e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '.shell.pre-oh-my-zsh',\n",
       " '.virtual_documents',\n",
       " '.zcompdump',\n",
       " '.zcompdump-featurize-5.8',\n",
       " '.python_history',\n",
       " '.server.log',\n",
       " '.nvm',\n",
       " '.ipython',\n",
       " '.vim',\n",
       " '.lesshst',\n",
       " '.jupyter',\n",
       " '.nv',\n",
       " '.oh-my-zsh',\n",
       " '.local',\n",
       " '.zshrc',\n",
       " '.bash_logout',\n",
       " '.profile',\n",
       " '.bashrc',\n",
       " '.npm',\n",
       " '.conda',\n",
       " '.keras',\n",
       " '.config',\n",
       " 'finetune-mistral-7B.ipynb',\n",
       " '.zsh_history',\n",
       " '.cache',\n",
       " '.ssh',\n",
       " '.yarn',\n",
       " '.wget-hsts',\n",
       " '.vscode-server',\n",
       " 'work',\n",
       " 'train-dir']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283fc51f-d5ec-4097-b6b9-7ee74bcccea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a4192b-7af8-45e5-88b0-20dd660a7e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-auth 2.22.0 requires urllib3<2.0, but you have urllib3 2.1.0 which is incompatible.\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch\n",
    "!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n",
    "!pip install -q datasets #huggingface datasets to download and manipulate datasets\n",
    "!pip install -q peft #Parameter efficient finetuning - for qLora Finetuning\n",
    "!pip install -q bitsandbytes #For Model weights quantisation\n",
    "!pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning\n",
    "!pip install -q wandb -U #Used to monitor the model score during training\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee394ad-e2ac-46e8-bcb4-ccd4b3cfe1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  3 01:24:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:10:00.0 Off |                  Off |\n",
      "| 30%   25C    P8     6W / 300W |      0MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26b513-3d0a-44ba-be27-ae4e9bc3a457",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d85483-e946-4849-ac74-a92dfa0cae42",
   "metadata": {},
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7057737-c054-4165-b5d8-bea1cf467483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 01:24:31.912598: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-03 01:24:32.589479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484f96d-427d-46cb-83e6-604d46b90534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ad2e238-0d7c-428c-9020-68bdc87ba24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Create a function to calculate the sum of a sequence of integers.',\n",
       " 'input': '[1, 2, 3, 4, 5]',\n",
       " 'output': '# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 样例数据\n",
    "{\n",
    "    \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n",
    "    \"input\":\"[1, 2, 3, 4, 5]\",\n",
    "    \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdf12c7e-fc95-4c96-9065-061481fce3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (replace 'your_dataset_name' and 'split_name' with your actual dataset information)\n",
    "# dataset = load_dataset(\"your_dataset_name\", split=\"split_name\")\n",
    "dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c50be84-fab6-4159-8a31-4b197d33ac5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "492bb4ac-90fa-4af4-9e64-233b86584022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to output the right formate for each row in the dataset\n",
    "def create_text_row(instruction, output, input):\n",
    "    text_row = f\"\"\"<s>[INST] {instruction} here are the inputs {input} [/INST] \\\\n {output} </s>\"\"\"\n",
    "    #text_row = f\"<s>[INST] {instruction} here are the inputs {input} [/INST] \\\\n {output} </s>\"\n",
    "    return text_row\n",
    "\n",
    "# interate over all the rows formate the dataset and store it in a jsonl file\n",
    "def process_jsonl_file(output_file_path):\n",
    "    with open(output_file_path, \"w\") as output_jsonl_file:\n",
    "        for item in dataset:\n",
    "            json_object = {\n",
    "                \"text\": create_text_row(item[\"instruction\"], item[\"input\"] ,item[\"output\"]),\n",
    "                \"instruction\": item[\"instruction\"],\n",
    "                \"input\": item[\"input\"],\n",
    "                \"output\": item[\"output\"]\n",
    "            }\n",
    "            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n",
    "\n",
    "# Provide the path where you want to save the formatted dataset\n",
    "process_jsonl_file(\"./training_dataset_2new.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d71f754c-5628-4d1c-86d9-4654f6747ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\\\\    # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum</s>',\n",
       " 'instruction': 'Create a function to calculate the sum of a sequence of integers',\n",
       " 'input': '[1, 2, 3, 4, 5]',\n",
       " 'output': '# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"text\":\"<s>[INST] Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] [/INST]\\\\\\\n",
    "    # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum</s>\",\n",
    "    \"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n",
    "    \"input\":\"[1, 2, 3, 4, 5]\",\n",
    "    \"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f816151f-8f72-4577-b2f1-7268814585c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73ac030b-3e9b-4e17-b0fa-06f887bb9dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open('training_dataset_2.jsonl') as reader:\n",
    "    for obj in reader:\n",
    "        # 处理每个 JSON 对象\n",
    "        print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a9a16d7-4ec9-470c-a537-0f6441df6d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 4466.78it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1063.73it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaInferenceError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1941\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1940\u001b[0m num_shards \u001b[38;5;241m=\u001b[39m shard_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1941\u001b[0m num_examples, num_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1942\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/datasets/arrow_writer.py:599\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SchemaInferenceError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass `features` or at least one example when writing data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    600\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone writing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    602\u001b[0m )\n",
      "\u001b[0;31mSchemaInferenceError\u001b[0m: Please pass `features` or at least one example when writing data",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## 训练集\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#train_dataset = load_dataset('json', data_files='./training_dataset_example.jsonl',# split='train',\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#                            features={'text': 'string', 'instruction': 'string', 'input': 'string', 'output': 'string'})\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./training_dataset_2.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/datasets/load.py:2152\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2152\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2163\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2164\u001b[0m )\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/datasets/builder.py:948\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1043\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1046\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1050\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1805\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1803\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1805\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1806\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1807\u001b[0m     ):\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1809\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1950\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, SchemaInferenceError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1949\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "## 训练集\n",
    "#train_dataset = load_dataset('json', data_files='./training_dataset_example.jsonl',# split='train',\n",
    "#                            features={'text': 'string', 'instruction': 'string', 'input': 'string', 'output': 'string'})\n",
    "train_dataset = load_dataset('json', data_files='./training_dataset_2.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c8fb1d4-1434-4dce-9b87-bfdf116a0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"mistralai-Code-Instruct\" #set the name of the new model\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da26ae67-2c2e-467d-9136-69a42336d19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 571/571 [00:00<00:00, 1.71MB/s]\n",
      "pytorch_model.bin.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 3.73MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   0%|          | 0.00/9.94G [00:00<?, ?B/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   0%|          | 41.9M/9.94G [00:00<00:26, 367MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   1%|          | 94.4M/9.94G [00:00<00:22, 446MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   2%|▏         | 157M/9.94G [00:00<00:19, 513MB/s] \u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   2%|▏         | 210M/9.94G [00:00<00:19, 498MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   3%|▎         | 283M/9.94G [00:00<00:19, 508MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   3%|▎         | 346M/9.94G [00:00<00:17, 537MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   4%|▍         | 409M/9.94G [00:00<00:17, 542MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   5%|▍         | 472M/9.94G [00:00<00:17, 536MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   5%|▌         | 545M/9.94G [00:01<00:16, 567MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   6%|▌         | 608M/9.94G [00:01<00:18, 509MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   7%|▋         | 661M/9.94G [00:01<00:18, 490MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   7%|▋         | 734M/9.94G [00:01<00:17, 531MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   8%|▊         | 797M/9.94G [00:01<00:17, 516MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   9%|▊         | 849M/9.94G [00:01<00:17, 512MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:   9%|▉         | 923M/9.94G [00:01<00:16, 548MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  10%|▉         | 986M/9.94G [00:01<00:20, 443MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  10%|█         | 1.04G/9.94G [00:02<00:19, 451MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  11%|█         | 1.10G/9.94G [00:02<00:18, 479MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  12%|█▏        | 1.15G/9.94G [00:02<00:18, 474MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  12%|█▏        | 1.22G/9.94G [00:02<00:17, 510MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  13%|█▎        | 1.28G/9.94G [00:02<00:16, 513MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  13%|█▎        | 1.33G/9.94G [00:02<00:17, 497MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  14%|█▍        | 1.38G/9.94G [00:02<00:17, 482MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  14%|█▍        | 1.44G/9.94G [00:02<00:17, 489MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  15%|█▌        | 1.50G/9.94G [00:02<00:16, 520MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  16%|█▌        | 1.56G/9.94G [00:03<00:16, 518MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  16%|█▌        | 1.61G/9.94G [00:03<00:16, 512MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  17%|█▋        | 1.68G/9.94G [00:03<00:15, 537MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  18%|█▊        | 1.74G/9.94G [00:03<00:15, 535MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  18%|█▊        | 1.80G/9.94G [00:03<00:15, 525MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  19%|█▉        | 1.87G/9.94G [00:03<00:14, 541MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  19%|█▉        | 1.93G/9.94G [00:03<00:15, 529MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  20%|██        | 1.99G/9.94G [00:03<00:15, 497MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  21%|██        | 2.06G/9.94G [00:04<00:14, 527MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  21%|██▏       | 2.12G/9.94G [00:04<00:14, 524MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  22%|██▏       | 2.18G/9.94G [00:04<00:14, 529MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  23%|██▎       | 2.24G/9.94G [00:04<00:14, 533MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  23%|██▎       | 2.31G/9.94G [00:04<00:13, 551MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  24%|██▍       | 2.37G/9.94G [00:04<00:13, 547MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  24%|██▍       | 2.43G/9.94G [00:04<00:13, 538MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  25%|██▌       | 2.50G/9.94G [00:04<00:13, 551MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  26%|██▌       | 2.56G/9.94G [00:04<00:13, 546MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  26%|██▋       | 2.62G/9.94G [00:05<00:13, 535MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  27%|██▋       | 2.68G/9.94G [00:05<00:13, 554MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  28%|██▊       | 2.75G/9.94G [00:05<00:13, 541MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  28%|██▊       | 2.81G/9.94G [00:05<00:14, 509MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  29%|██▉       | 2.87G/9.94G [00:05<00:13, 535MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  30%|██▉       | 2.94G/9.94G [00:05<00:13, 537MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  30%|███       | 3.00G/9.94G [00:05<00:12, 534MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  31%|███       | 3.06G/9.94G [00:05<00:12, 533MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  31%|███▏      | 3.12G/9.94G [00:06<00:12, 553MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  32%|███▏      | 3.19G/9.94G [00:06<00:12, 553MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  33%|███▎      | 3.25G/9.94G [00:06<00:12, 547MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  33%|███▎      | 3.31G/9.94G [00:06<00:11, 561MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  34%|███▍      | 3.38G/9.94G [00:06<00:11, 549MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  35%|███▍      | 3.44G/9.94G [00:06<00:11, 549MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  35%|███▌      | 3.50G/9.94G [00:06<00:11, 558MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  36%|███▌      | 3.57G/9.94G [00:06<00:12, 519MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  36%|███▋      | 3.63G/9.94G [00:06<00:12, 498MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  37%|███▋      | 3.69G/9.94G [00:07<00:11, 522MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  38%|███▊      | 3.75G/9.94G [00:07<00:12, 480MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  38%|███▊      | 3.82G/9.94G [00:07<00:12, 509MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  39%|███▉      | 3.88G/9.94G [00:07<00:11, 526MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  40%|███▉      | 3.94G/9.94G [00:07<00:11, 509MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  40%|████      | 4.01G/9.94G [00:07<00:11, 532MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  41%|████      | 4.07G/9.94G [00:07<00:10, 536MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  42%|████▏     | 4.13G/9.94G [00:07<00:10, 537MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  42%|████▏     | 4.19G/9.94G [00:08<00:10, 553MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  43%|████▎     | 4.26G/9.94G [00:08<00:10, 539MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  43%|████▎     | 4.32G/9.94G [00:08<00:10, 535MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  44%|████▍     | 4.38G/9.94G [00:08<00:10, 516MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  45%|████▍     | 4.44G/9.94G [00:08<00:10, 508MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  45%|████▌     | 4.50G/9.94G [00:08<00:10, 516MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  46%|████▌     | 4.55G/9.94G [00:08<00:10, 511MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  46%|████▋     | 4.61G/9.94G [00:08<00:09, 539MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  47%|████▋     | 4.68G/9.94G [00:08<00:09, 527MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  48%|████▊     | 4.74G/9.94G [00:09<00:09, 533MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  48%|████▊     | 4.80G/9.94G [00:09<00:09, 529MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  49%|████▉     | 4.87G/9.94G [00:09<00:09, 548MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  50%|████▉     | 4.93G/9.94G [00:09<00:09, 541MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  50%|█████     | 4.99G/9.94G [00:09<00:09, 540MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  51%|█████     | 5.05G/9.94G [00:09<00:08, 555MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  51%|█████▏    | 5.12G/9.94G [00:09<00:09, 534MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  52%|█████▏    | 5.18G/9.94G [00:09<00:08, 534MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  53%|█████▎    | 5.24G/9.94G [00:09<00:08, 552MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  53%|█████▎    | 5.31G/9.94G [00:10<00:08, 565MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  54%|█████▍    | 5.37G/9.94G [00:10<00:08, 571MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  55%|█████▍    | 5.43G/9.94G [00:10<00:07, 573MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  55%|█████▌    | 5.49G/9.94G [00:10<00:07, 582MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  56%|█████▌    | 5.56G/9.94G [00:10<00:07, 591MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  57%|█████▋    | 5.62G/9.94G [00:10<00:07, 581MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  57%|█████▋    | 5.68G/9.94G [00:10<00:07, 587MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  58%|█████▊    | 5.75G/9.94G [00:10<00:07, 576MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  58%|█████▊    | 5.81G/9.94G [00:10<00:07, 567MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  59%|█████▉    | 5.87G/9.94G [00:11<00:07, 565MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  60%|█████▉    | 5.93G/9.94G [00:11<00:16, 246MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  60%|██████    | 6.00G/9.94G [00:11<00:13, 301MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  61%|██████    | 6.07G/9.94G [00:11<00:10, 370MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  62%|██████▏   | 6.14G/9.94G [00:11<00:08, 429MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  62%|██████▏   | 6.21G/9.94G [00:12<00:08, 451MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  63%|██████▎   | 6.27G/9.94G [00:12<00:11, 307MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  64%|██████▎   | 6.33G/9.94G [00:12<00:09, 361MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  64%|██████▍   | 6.41G/9.94G [00:12<00:08, 419MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  65%|██████▌   | 6.48G/9.94G [00:12<00:07, 472MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  66%|██████▌   | 6.55G/9.94G [00:12<00:06, 513MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  67%|██████▋   | 6.62G/9.94G [00:13<00:06, 539MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  67%|██████▋   | 6.68G/9.94G [00:14<00:28, 114MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  68%|██████▊   | 6.75G/9.94G [00:14<00:20, 155MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  69%|██████▊   | 6.83G/9.94G [00:14<00:15, 205MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  69%|██████▉   | 6.90G/9.94G [00:15<00:11, 261MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  70%|███████   | 6.97G/9.94G [00:15<00:09, 319MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  71%|███████   | 7.05G/9.94G [00:15<00:07, 376MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  72%|███████▏  | 7.12G/9.94G [00:15<00:06, 434MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  72%|███████▏  | 7.19G/9.94G [00:15<00:05, 485MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  73%|███████▎  | 7.27G/9.94G [00:15<00:05, 533MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  74%|███████▍  | 7.34G/9.94G [00:15<00:04, 570MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  75%|███████▍  | 7.41G/9.94G [00:15<00:04, 599MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  75%|███████▌  | 7.49G/9.94G [00:15<00:03, 626MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  76%|███████▌  | 7.56G/9.94G [00:15<00:03, 640MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  77%|███████▋  | 7.63G/9.94G [00:16<00:03, 616MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  78%|███████▊  | 7.71G/9.94G [00:16<00:03, 636MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  78%|███████▊  | 7.78G/9.94G [00:16<00:03, 649MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  79%|███████▉  | 7.85G/9.94G [00:16<00:03, 666MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  80%|███████▉  | 7.93G/9.94G [00:16<00:03, 663MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  80%|████████  | 8.00G/9.94G [00:16<00:02, 668MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  81%|████████  | 8.07G/9.94G [00:16<00:02, 669MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  82%|████████▏ | 8.15G/9.94G [00:16<00:02, 673MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  83%|████████▎ | 8.22G/9.94G [00:16<00:02, 680MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  83%|████████▎ | 8.29G/9.94G [00:17<00:02, 676MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  84%|████████▍ | 8.37G/9.94G [00:17<00:03, 476MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  85%|████████▍ | 8.44G/9.94G [00:17<00:02, 526MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  86%|████████▌ | 8.51G/9.94G [00:17<00:02, 569MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  86%|████████▋ | 8.59G/9.94G [00:17<00:02, 591MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  87%|████████▋ | 8.66G/9.94G [00:17<00:02, 611MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  88%|████████▊ | 8.73G/9.94G [00:17<00:01, 622MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  89%|████████▊ | 8.81G/9.94G [00:18<00:01, 634MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  89%|████████▉ | 8.88G/9.94G [00:18<00:01, 645MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  90%|█████████ | 8.95G/9.94G [00:18<00:01, 652MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  91%|█████████ | 9.03G/9.94G [00:18<00:01, 651MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  92%|█████████▏| 9.10G/9.94G [00:18<00:01, 653MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  92%|█████████▏| 9.18G/9.94G [00:18<00:01, 657MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  93%|█████████▎| 9.25G/9.94G [00:18<00:01, 665MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  94%|█████████▍| 9.32G/9.94G [00:18<00:00, 669MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  94%|█████████▍| 9.40G/9.94G [00:18<00:00, 669MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  95%|█████████▌| 9.47G/9.94G [00:19<00:00, 664MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  96%|█████████▌| 9.54G/9.94G [00:19<00:00, 664MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  97%|█████████▋| 9.62G/9.94G [00:19<00:00, 591MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  97%|█████████▋| 9.68G/9.94G [00:20<00:01, 205MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  98%|█████████▊| 9.75G/9.94G [00:20<00:00, 261MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX:  99%|█████████▉| 9.83G/9.94G [00:20<00:00, 319MB/s]\u001b[A\n",
      "(…)hjTZRWN16W9g__&Key-Pair-Id=KVTP0A1DKRTAX: 100%|██████████| 9.94G/9.94G [00:20<00:00, 484MB/s]\u001b[A\n",
      "Downloading shards:  50%|█████     | 1/2 [00:20<00:20, 20.88s/it]\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:   0%|          | 0.00/5.06G [00:00<?, ?B/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:   1%|          | 52.4M/5.06G [00:00<00:10, 501MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:   2%|▏         | 105M/5.06G [00:00<00:10, 478MB/s] \u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:   4%|▎         | 178M/5.06G [00:00<00:08, 558MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:   5%|▍         | 241M/5.06G [00:00<00:08, 539MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:   6%|▌         | 304M/5.06G [00:00<00:09, 529MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:   7%|▋         | 377M/5.06G [00:00<00:08, 527MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:   9%|▉         | 451M/5.06G [00:00<00:08, 568MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  10%|█         | 514M/5.06G [00:00<00:08, 557MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  11%|█▏        | 577M/5.06G [00:01<00:08, 544MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  13%|█▎        | 640M/5.06G [00:01<00:08, 513MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  14%|█▎        | 692M/5.06G [00:01<00:09, 480MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  15%|█▍        | 744M/5.06G [00:01<00:08, 490MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  16%|█▌        | 818M/5.06G [00:01<00:07, 540MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  17%|█▋        | 881M/5.06G [00:01<00:07, 528MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  19%|█▊        | 944M/5.06G [00:01<00:08, 480MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  20%|██        | 1.02G/5.06G [00:01<00:07, 525MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  21%|██▏       | 1.08G/5.06G [00:02<00:07, 520MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  23%|██▎       | 1.14G/5.06G [00:02<00:07, 514MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  24%|██▍       | 1.22G/5.06G [00:02<00:07, 512MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  25%|██▌       | 1.29G/5.06G [00:02<00:06, 546MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  27%|██▋       | 1.35G/5.06G [00:02<00:08, 453MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  28%|██▊       | 1.43G/5.06G [00:02<00:07, 474MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  30%|██▉       | 1.50G/5.06G [00:02<00:06, 518MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  31%|███       | 1.56G/5.06G [00:03<00:06, 520MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  32%|███▏      | 1.63G/5.06G [00:03<00:06, 521MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  33%|███▎      | 1.69G/5.06G [00:03<00:06, 541MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  35%|███▍      | 1.75G/5.06G [00:03<00:06, 506MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  36%|███▌      | 1.80G/5.06G [00:03<00:06, 508MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  37%|███▋      | 1.88G/5.06G [00:03<00:05, 555MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  38%|███▊      | 1.94G/5.06G [00:03<00:05, 549MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  40%|███▉      | 2.00G/5.06G [00:03<00:05, 542MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  41%|████      | 2.07G/5.06G [00:03<00:05, 538MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  42%|████▏     | 2.14G/5.06G [00:04<00:05, 584MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  43%|████▎     | 2.20G/5.06G [00:04<00:05, 558MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  45%|████▍     | 2.26G/5.06G [00:04<00:05, 557MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  46%|████▌     | 2.33G/5.06G [00:04<00:05, 518MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  47%|████▋     | 2.40G/5.06G [00:04<00:04, 562MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  49%|████▊     | 2.46G/5.06G [00:04<00:04, 555MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  50%|████▉     | 2.53G/5.06G [00:04<00:04, 536MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  51%|█████     | 2.59G/5.06G [00:04<00:04, 515MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  52%|█████▏    | 2.64G/5.06G [00:05<00:05, 477MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  53%|█████▎    | 2.69G/5.06G [00:05<00:05, 462MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  54%|█████▍    | 2.75G/5.06G [00:05<00:05, 462MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  55%|█████▌    | 2.80G/5.06G [00:05<00:05, 448MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  56%|█████▋    | 2.85G/5.06G [00:05<00:04, 453MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  57%|█████▋    | 2.90G/5.06G [00:05<00:04, 466MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  58%|█████▊    | 2.96G/5.06G [00:05<00:04, 456MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  60%|█████▉    | 3.02G/5.06G [00:05<00:04, 486MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  61%|██████    | 3.07G/5.06G [00:05<00:04, 486MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  62%|██████▏   | 3.12G/5.06G [00:06<00:04, 457MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  63%|██████▎   | 3.19G/5.06G [00:06<00:03, 477MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  64%|██████▍   | 3.24G/5.06G [00:06<00:04, 448MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  65%|██████▌   | 3.30G/5.06G [00:06<00:03, 493MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  66%|██████▌   | 3.36G/5.06G [00:06<00:03, 500MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  67%|██████▋   | 3.41G/5.06G [00:06<00:03, 480MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  68%|██████▊   | 3.46G/5.06G [00:06<00:04, 400MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  69%|██████▉   | 3.51G/5.06G [00:07<00:04, 349MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  71%|███████   | 3.58G/5.06G [00:07<00:03, 397MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  72%|███████▏  | 3.63G/5.06G [00:07<00:03, 416MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  73%|███████▎  | 3.68G/5.06G [00:07<00:03, 416MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  74%|███████▎  | 3.73G/5.06G [00:07<00:03, 414MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  75%|███████▍  | 3.79G/5.06G [00:07<00:03, 421MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  76%|███████▌  | 3.84G/5.06G [00:07<00:02, 431MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  77%|███████▋  | 3.90G/5.06G [00:07<00:02, 467MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  78%|███████▊  | 3.95G/5.06G [00:08<00:02, 476MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  79%|███████▉  | 4.02G/5.06G [00:08<00:02, 478MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  80%|████████  | 4.07G/5.06G [00:08<00:02, 490MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  81%|████████▏ | 4.12G/5.06G [00:08<00:02, 454MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  83%|████████▎ | 4.19G/5.06G [00:08<00:01, 486MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  84%|████████▍ | 4.25G/5.06G [00:08<00:01, 494MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  85%|████████▍ | 4.30G/5.06G [00:08<00:01, 490MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  86%|████████▌ | 4.36G/5.06G [00:08<00:01, 490MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  87%|████████▋ | 4.41G/5.06G [00:08<00:01, 493MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  88%|████████▊ | 4.48G/5.06G [00:09<00:01, 497MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  90%|████████▉ | 4.54G/5.06G [00:09<00:01, 502MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  91%|█████████ | 4.59G/5.06G [00:09<00:01, 393MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  92%|█████████▏| 4.65G/5.06G [00:09<00:01, 345MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  93%|█████████▎| 4.69G/5.06G [00:09<00:01, 325MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  93%|█████████▎| 4.73G/5.06G [00:09<00:01, 309MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  94%|█████████▍| 4.77G/5.06G [00:10<00:00, 306MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  95%|█████████▌| 4.81G/5.06G [00:10<00:00, 304MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  96%|█████████▌| 4.84G/5.06G [00:10<00:00, 301MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  96%|█████████▋| 4.88G/5.06G [00:10<00:00, 304MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  97%|█████████▋| 4.91G/5.06G [00:10<00:00, 303MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  98%|█████████▊| 4.94G/5.06G [00:10<00:00, 304MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  98%|█████████▊| 4.97G/5.06G [00:10<00:00, 306MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX:  99%|█████████▉| 5.01G/5.06G [00:10<00:00, 313MB/s]\u001b[A\n",
      "(…)O2Lmgf%7Ek4A__&Key-Pair-Id=KVTP0A1DKRTAX: 100%|██████████| 5.06G/5.06G [00:11<00:00, 459MB/s]\u001b[A\n",
      "Downloading shards: 100%|██████████| 2/2 [00:32<00:00, 16.11s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.26s/it]\n",
      "generation_config.json: 100%|██████████| 116/116 [00:00<00:00, 356kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.47k/1.47k [00:00<00:00, 4.66MB/s]\n",
      "(…)PXnlEwy54vgQ__&Key-Pair-Id=KVTP0A1DKRTAX: 100%|██████████| 493k/493k [00:00<00:00, 21.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 5.28MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 216kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Load the base model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# Load MitsralAi tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c01ddd52-95a9-46c5-a6a4-4b15bd1f5642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print hello world in python c and c++\n",
      "\n",
      "Python:\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "C:\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main() {\n",
      "    printf(\"Hello, World!\\n\");\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "C++:\n",
      "```c++\n",
      "#include <iostream>\n",
      "\n",
      "int main() {\n",
      "    std::cout << \"Hello, World!\\n\";\n",
      "    return 0;\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"Print hello world in python c and c++\"\"\"\n",
    "\n",
    "# import random\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ec21a-3577-4052-9290-9115e569eb4f",
   "metadata": {},
   "source": [
    "### Fine-Tuning with qLora and Supervised Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e73821c9-6d4e-402c-bee9-4fe3bca5c371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 121959/121959 [00:11<00:00, 10193.89 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Set LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=100, # the total number of training steps to perform\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer for fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,  # You can specify the maximum sequence length here\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bdd10-441f-4880-8a06-702cf029a46c",
   "metadata": {},
   "source": [
    "### Lets start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "350bc43e-a6e4-4cdd-bb2a-d906105589ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81ae0d9d-e3ab-4b64-9abc-fa279e929bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 47.54 GiB total capacity; 45.98 GiB already allocated; 42.00 MiB free; 46.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start the training process\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(new_model)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:1530\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:1844\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1844\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1847\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1850\u001b[0m ):\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2701\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2700\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2701\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2704\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2724\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2723\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2724\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2726\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/peft/peft_model.py:1003\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[1;32m    994\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    995\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1001\u001b[0m         )\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:107\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1007\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1020\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:895\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    885\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    886\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    887\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m         use_cache,\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:624\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:257\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    256\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, seq_len\u001b[38;5;241m=\u001b[39mkv_seq_len)\n\u001b[0;32m--> 257\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# reuse k, v, self_attention\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/environment/miniconda3/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:158\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    156\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    157\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 158\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m)\n\u001b[1;32m    159\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 47.54 GiB total capacity; 45.98 GiB already allocated; 42.00 MiB free; 46.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc2dc9-3533-4b58-9722-850ab57cf15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c4711-bb8d-4120-bfb0-af101c63a0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c02271-6bd5-427d-b1a6-be881fd9ec66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd449eca-9505-4e5a-8f52-97dcfc1466b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16136b28-d133-4c2f-9f07-e9ce1476c94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
